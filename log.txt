
*****
3-11-23
Added check for Inf. Needed to change starting point for components without lower or upper bounds.
Added 1+ in denominator of primal and dual feasibility. This avoid issues with zero obj or rhs.
Added calculation of dual objective function and potential stopping criterion based on scaled duality gap. It looks like sometimes it may not work properly. Modified screen output accordingly.
Moved some parameters to IPM_const.h. It's easier to control the algorithm.
Moved getNonzeros after permutation is finished, so that it uses the fictitious matrix A with all entries set to one. This avoids that certain nonzeros are ignore because too small during the call to computeAThetaAT.
---
stat96v2.mps shows strange behaviour. Stepsizes go to zero and algorithm stagnates. Before that, primal infeas grows with stepwise 0.99. Need to check the residuals of the linear system to see if direction is accurate. 
---
Computation of Schur complement remains slow. Major contribution is the forward solve with all the columns. Need to solve all the columns at once, rather than one at a time. May bring small advantage. For neos-5052403-cygnet.mps, we are competitive with ipx, without parallelization implemented.
---

*****
6-11-23
Added computation of the residuals of the linear system. They appear low for all problems, including stat96v2.mps, but I don't know if they are low enough. It's difficult to obtain a relative measure, because the corresponding res_j may be very close to zero. Absolute values of infinity norms of the differences are of the order 1e-4 or lower. Maybe this is not low enough and needs some refinement after the solve. 
---
For neos-5052403-cygnet.mps with normal equations, most of the time is taken by building the normal equations at each iteration, because the Schur complement has size zero. Using Metis with normal equations corresponds to slicing matrix A into blocks of rows:

     [ A1 ]                                       [ A1*A1^T   0     A1*A3^T ]
 A = [ A2 ]    A^T = [ A1^T A2^T A3^T]    A*A^T = [   0     A2*A2^T A2*A3^T ]
     [ A3 ]                                       [ A3*A1^T A3*A2^T A3*A3^T ]

such that A1*A2^T is the zero matrix. Rather than computing A*A^T every time, I can compute all the five (or 2*nparts+1) nonzero blocks of A*A^T in parallel, using the previously stored slices of rows of A and the new Theta vector.
This requires storing the slices row and column wise. Also, computing Aj*Aj^T exploits symmetry, but computing Ai*Aj^T does not. Is this an issue?
---
Implemented forward/diagonal solve for all columns at once. It bring substantial benefit. E.g.
25fv47 with normal equations :  0.65 -> 0.23
25fv47 with augmented system :  2.66 -> 0.68 
80bau3b with normal equations:  1.80 -> 1.04
80bau3b with augmented system: 12.56 -> 4.09
Need to understand exactly how solving all rhs together brings such large benefit, to implement it later on in our factorisation.
Added warning that debug is on for large problems, as it was hampering performance before I noticed.
---
Need to add LU factorisation from HFactor. Need to create function, to be called from Metis_caller::factor(), that computes contribution to the Schur complement for a given linking and diagonal block, to improve modularity. Then, Metis_caller::factor() does not have to change, and I can add any method to compute the contribution to the Schur complement.
---


*****
7-11-23
Factorization of the schur complement switched to dense Lapack routine, using dsytrf and dsytrs. 
Avoid converting schur complement from dense to spase and using inefficient sparse factorization for it.
---
Added checks for empty schur complement, otherwise lapack complains.
Separated computation of contributions to schur complement from Metis_caller::factor(). Now they are in a separate file and one can add any method he wants.
Added HFactor as general solver (not for metis yet). It seems to be considerably slower than MA86. Julian's experiments seemed to indicate the opposite, not sure what is happening here. Also, it fails in a weird way for 80bau3b with augmented system.
---
It looks like MA86 does not produce an elimination ordering. That's weird because the code is quite performant. Checking the number of nonzero in the computed factor and comparing with the factor that Matlab produces, it looks like indeed there is large margin of improvement. 
I need to use HSL_MC68 to produce a good elimination ordering to pass to MA86. I hope it works without using Metis, otherwise I need to switch to the old Metis 4 and change the interface.
---


*****
8-11-23
Inserted MC68 to compute the elimination ordering. It reduces the number of nonzeros in the L factor considerably (5 times for 80bau3b). Times improve considerably.
For neos-5052403-cygnet the effect is much smaller.
---
Extra experiments using HFactor. 
For 80bau3b, L from MA86 has 107k entries, L and U together from HFactor have 75k entries. 
Solve time is slightly reduced due to this. However, factorization time is much larger (0.05 vs 0.75).
For neos-5052403-cygnet, starting point not yet computed after 5 minutes. MA86 converges in 15s.
For stat96v2, L from MA86 has 1.7M entries, LU from HFactor have 4.2M entries. Factorization is much slower.
---
Need to run the code on the whole Netlib collection. Maybe compare times of MA86 and HFactor. 
Need a master file to run all problems.


*****
9-11-23
Computed netlib results for various combinations of type of linear system, regularization and tolerance. 
Normal equations is usually faster, but fails to converge for come problems. Augmented system without regularization converges for all 96 problems.
Regularization seems to only make things worse. Need specialized dynamic regularization within the factorization.
Added extraction of problem name from path. Added print of problem name to screen.
---


*****
10-11-23
Managed to make solution with Metis blocks work with HFactor. Needed to use the correct permutation at the correct point in the code. 
Needed also to make ftranL and btranU public in HFactor.h.
I still need to make extensive tests, but it looks like for some problems it brings some benefit. 
However, for other problems it is incredibly slow and sometimes it just fails. It goes well until at some point the direction is not correct anymore and the iterations start diverging. Weird.
I don't understand why sometimes the starting point alone takes a lot of time. Need to switch the starting point to use metis blocks as well.
---
For next week, I need to understand how to make the code parallel and do some more tests to check when HFactor is better.


*****
13-11-23
Changed starting point to use metis blocks. 
When using normal equations, minimal changes needed. 
When using augmented system, solution of A*A^T * q = s + A*r is found by solving
    [ -I  A^T] [ p ] = [ r ]
    [  A   0 ] [ q ] = [ s ]
and ignoring the solution p. I changed this also for the non-metis augmented system, to avoid forming A*A^T (it may be much denser).
---
Added break in main loop is an error occurs during factorization (any type, normal equation, augmented, metis or not). 
Weird behaviour for some problems seems to be caused by error in factorization. Incomplete of wrong data is used to solve the following linear systems.
---


*****
14-11-23
Added parallel stuff from highs::parallel. Code works, but it is not faster. I have to make local copies to avoid race conditions and then use a highs::parallel::mutex to write to the shared variables.
Surprisingly, mutex or not, the code works anyway. I don't understand why. It should happen sometimes that the schur complement is missing something.
Checking the times, the threads indeed do run in parallel, but executing a thread with only one part takes longer than executing one thread in single threaded mode. That's weird. They should take the same amount of time. 
Overall, the total time does not change much because of this. Maybe I am using the mutex wrong, or maybe I should compile in a different way to exploit the threads in the best way. I need to ask Julian.
---
I think I got it. MA86 is already using openmp for parallelism. It may use it for examples to solve multiple rhs at the same time. 
But anyway, it uses multiple threads already and using std::thread (through the highs::parallel interface) messes things up.
Two parts are done together, but each one uses half the number of threads that it would use without highs::parallel. So, nothing changes. 
Using the LU factorization of HFactor, which is not parallel, there is a difference of approximately a factor 2 (as expected for 2 blocks).
---
I am changing also the solve() routine to make it parallel. This has a smaller impact than factor, but it's worth doing it as well. 
I had to make minor changes. I precomputed blockStart, rather that computing sequentially the starting point of each block (should have done it already).
Code works.
---
A summary of where the code has been made parallel. 

The linear system is:
  [A_1             B_1^T]   [x_1]     [y_1]
  [    A_2         B_2^T]   [x_2]     [y_2]
  [        ...          ] * [...]  =  [...]
  [            A_k B_k^T]   [x_k]     [y_k]
  [B_1 B_2 ... B_k  A_S ]   [x_S]     [y_S]

In Metis_caller::factor() :
  - parallel: factorization of A_j, j = 1,...,k
  - parallel: forming Schur contribution, S_j = B_j * A_j^{-1} * B_j^T, j = 1,...,k
  - serial: assemblying Schur complement, S = A_S - \sum S_j (through mutex)
  - serial: factorization of S

In Metis_caller::solve() :
  - parallel: forming contribution to Schur rhs, z_j = B_j * A_j^{-1} * y_j, j = 1,...,k
  - serial: assemblying Schur rhs, z_S = y_S - \sum z_j (through mutex)
  - serial: solving for x_S = S^{-1} * z_S
  - parallel: solving for x_j = A_j^{-1} * (y_j - B_j^T * x_S), j = 1,...,k

---

