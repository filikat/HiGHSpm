
*****
3-11-23
Added check for Inf. Needed to change starting point for components without lower or upper bounds.
Added 1+ in denominator of primal and dual feasibility. This avoid issues with zero obj or rhs.
Added calculation of dual objective function and potential stopping criterion based on scaled duality gap. It looks like sometimes it may not work properly. Modified screen output accordingly.
Moved some parameters to IPM_const.h. It's easier to control the algorithm.
Moved getNonzeros after permutation is finished, so that it uses the fictitious matrix A with all entries set to one. This avoids that certain nonzeros are ignore because too small during the call to computeAThetaAT.
---
stat96v2.mps shows strange behaviour. Stepsizes go to zero and algorithm stagnates. Before that, primal infeas grows with stepwise 0.99. Need to check the residuals of the linear system to see if direction is accurate. 
---
Computation of Schur complement remains slow. Major contribution is the forward solve with all the columns. Need to solve all the columns at once, rather than one at a time. May bring small advantage. For neos-5052403-cygnet.mps, we are competitive with ipx, without parallelization implemented.
---

*****
6-11-23
Added computation of the residuals of the linear system. They appear low for all problems, including stat96v2.mps, but I don't know if they are low enough. It's difficult to obtain a relative measure, because the corresponding res_j may be very close to zero. Absolute values of infinity norms of the differences are of the order 1e-4 or lower. Maybe this is not low enough and needs some refinement after the solve. 
---
For neos-5052403-cygnet.mps with normal equations, most of the time is taken by building the normal equations at each iteration, because the Schur complement has size zero. Using Metis with normal equations corresponds to slicing matrix A into blocks of rows:

     [ A1 ]                                       [ A1*A1^T   0     A1*A3^T ]
 A = [ A2 ]    A^T = [ A1^T A2^T A3^T]    A*A^T = [   0     A2*A2^T A2*A3^T ]
     [ A3 ]                                       [ A3*A1^T A3*A2^T A3*A3^T ]

such that A1*A2^T is the zero matrix. Rather than computing A*A^T every time, I can compute all the five (or 2*nparts+1) nonzero blocks of A*A^T in parallel, using the previously stored slices of rows of A and the new Theta vector.
This requires storing the slices row and column wise. Also, computing Aj*Aj^T exploits symmetry, but computing Ai*Aj^T does not. Is this an issue?
---
Implemented forward/diagonal solve for all columns at once. It bring substantial benefit. E.g.
25fv47 with normal equations :  0.65 -> 0.23
25fv47 with augmented system :  2.66 -> 0.68 
80bau3b with normal equations:  1.80 -> 1.04
80bau3b with augmented system: 12.56 -> 4.09
Need to understand exactly how solving all rhs together brings such large benefit, to implement it later on in our factorisation.
Added warning that debug is on for large problems, as it was hampering performance before I noticed.
---
Need to add LU factorisation from HFactor. Need to create function, to be called from Metis_caller::factor(), that computes contribution to the Schur complement for a given linking and diagonal block, to improve modularity. Then, Metis_caller::factor() does not have to change, and I can add any method to compute the contribution to the Schur complement.
---


*****
7-11-23
Factorization of the schur complement switched to dense Lapack routine, using dsytrf and dsytrs. 
Avoid converting schur complement from dense to spase and using inefficient sparse factorization for it.
---
Added checks for empty schur complement, otherwise lapack complains.
Separated computation of contributions to schur complement from Metis_caller::factor(). Now they are in a separate file and one can add any method he wants.
Added HFactor as general solver (not for metis yet). It seems to be considerably slower than MA86. Julian's experiments seemed to indicate the opposite, not sure what is happening here. Also, it fails in a weird way for 80bau3b with augmented system.
---
It looks like MA86 does not produce an elimination ordering. That's weird because the code is quite performant. Checking the number of nonzero in the computed factor and comparing with the factor that Matlab produces, it looks like indeed there is large margin of improvement. 
I need to use HSL_MC68 to produce a good elimination ordering to pass to MA86. I hope it works without using Metis, otherwise I need to switch to the old Metis 4 and change the interface.
---


*****
8-11-23
Inserted MC68 to compute the elimination ordering. It reduces the number of nonzeros in the L factor considerably (5 times for 80bau3b). Times improve considerably.
For neos-5052403-cygnet the effect is much smaller.
---
Extra experiments using HFactor. 
For 80bau3b, L from MA86 has 107k entries, L and U together from HFactor have 75k entries. 
Solve time is slightly reduced due to this. However, factorization time is much larger (0.05 vs 0.75).
For neos-5052403-cygnet, starting point not yet computed after 5 minutes. MA86 converges in 15s.
For stat96v2, L from MA86 has 1.7M entries, LU from HFactor have 4.2M entries. Factorization is much slower.
---
Need to run the code on the whole Netlib collection. Maybe compare times of MA86 and HFactor. 
Need a master file to run all problems.


*****
9-11-23
Computed netlib results for various combinations of type of linear system, regularization and tolerance. 
Normal equations is usually faster, but fails to converge for come problems. Augmented system without regularization converges for all 96 problems.
Regularization seems to only make things worse. Need specialized dynamic regularization within the factorization.
Added extraction of problem name from path. Added print of problem name to screen.
---


*****
10-11-23
Managed to make solution with Metis blocks work with HFactor. Needed to use the correct permutation at the correct point in the code. 
Needed also to make ftranL and btranU public in HFactor.h.
I still need to make extensive tests, but it looks like for some problems it brings some benefit. 
However, for other problems it is incredibly slow and sometimes it just fails. It goes well until at some point the direction is not correct anymore and the iterations start diverging. Weird.
I don't understand why sometimes the starting point alone takes a lot of time. Need to switch the starting point to use metis blocks as well.
---
For next week, I need to understand how to make the code parallel and do some more tests to check when HFactor is better.


*****
13-11-23
Changed starting point to use metis blocks. 
When using normal equations, minimal changes needed. 
When using augmented system, solution of A*A^T * q = s + A*r is found by solving
    [ -I  A^T] [ p ] = [ r ]
    [  A   0 ] [ q ] = [ s ]
and ignoring the solution p. I changed this also for the non-metis augmented system, to avoid forming A*A^T (it may be much denser).
---
Added break in main loop is an error occurs during factorization (any type, normal equation, augmented, metis or not). 
Weird behaviour for some problems seems to be caused by error in factorization. Incomplete of wrong data is used to solve the following linear systems.
---


*****
14-11-23
Added parallel stuff from highs::parallel. Code works, but it is not faster. I have to make local copies to avoid race conditions and then use a highs::parallel::mutex to write to the shared variables.
Surprisingly, mutex or not, the code works anyway. I don't understand why. It should happen sometimes that the schur complement is missing something.
Checking the times, the threads indeed do run in parallel, but executing a thread with only one part takes longer than executing one thread in single threaded mode. That's weird. They should take the same amount of time. 
Overall, the total time does not change much because of this. Maybe I am using the mutex wrong, or maybe I should compile in a different way to exploit the threads in the best way. I need to ask Julian.
---
I think I got it. MA86 is already using openmp for parallelism. It may use it for examples to solve multiple rhs at the same time. 
But anyway, it uses multiple threads already and using std::thread (through the highs::parallel interface) messes things up.
Two parts are done together, but each one uses half the number of threads that it would use without highs::parallel. So, nothing changes. 
Using the LU factorization of HFactor, which is not parallel, there is a difference of approximately a factor 2 (as expected for 2 blocks).
---
I am changing also the solve() routine to make it parallel. This has a smaller impact than factor, but it's worth doing it as well. 
I had to make minor changes. I precomputed blockStart, rather that computing sequentially the starting point of each block (should have done it already).
Code works.
---
A summary of where the code has been made parallel. 

The linear system is:
  [A_1             B_1^T]   [x_1]     [y_1]
  [    A_2         B_2^T]   [x_2]     [y_2]
  [        ...          ] * [...]  =  [...]
  [            A_k B_k^T]   [x_k]     [y_k]
  [B_1 B_2 ... B_k  A_S ]   [x_S]     [y_S]

In Metis_caller::factor() :
  - parallel: factorization of A_j, j = 1,...,k
  - parallel: forming Schur contribution, S_j = B_j * A_j^{-1} * B_j^T, j = 1,...,k
  - serial: assemblying Schur complement, S = A_S - \sum S_j (through mutex)
  - serial: factorization of S

In Metis_caller::solve() :
  - parallel: forming contribution to Schur rhs, z_j = B_j * A_j^{-1} * y_j, j = 1,...,k
  - serial: assemblying Schur rhs, z_S = y_S - \sum z_j (through mutex)
  - serial: solving for x_S = S^{-1} * z_S
  - parallel: solving for x_j = A_j^{-1} * (y_j - B_j^T * x_S), j = 1,...,k

---


*****
17-11-23
I tried to switch off openmp when using MA86 to obtain more meaningful results. It's not clear how to do it.
I tried to remove the -fopenmp flag from the makefile, but this does not change much. Maybe due to how I have to link omp on Mac. Maybe due to something else that MA86 does.
I can set the OMP_NUM_THREADS environment variable to 1 to force omp to work on a single thread. This gives more sensible results.
I report some times for 80bau3b with solvers 5 or 6 (i.e. Metis with augmented system and Metis with normal equations, resp.)

solver    OMP_NUM_THREADS     serial       parallel
5              1              2.1          1.3
6              1              0.4          0.3
5              2              1.7          1.1
6              2              0.4          0.3
5              4              1.8          1.4
6              4              0.4          0.4
5              /              2.4          3.3
6              /              0.6          0.9

The weird thing is that if I don't set OMP_NUM_THREADS (last two rows), the times are much larger. It should automatically choose the number of threads, but something seems to be going wrong in this case.
If I force the number of threads, then the behaviour is somewhat sensible: for the serial code (i.e. without highs::parallel), the times is reduced when I use more than one thread, because MA86 already exploits parallelism.
For the parallel code (i.e. with highs::parallel), the times does not change much, but it gets slightly worse when I increase the number of threads (maybe because the two parallel stuff compete with each other and somehow this messes things up).
Looking at the time taken by each block in the partition, when using 1 thread only, with solver=5, the results make sense:
the serial code takes roughly 2e-2 sec to assemble each part of the schur complement, thus the total assembly time is roughly 4e-2.
the parallel code takes roughly 2.2e-2 to assemble each part (a bit more than the serial, I guess some overhead from highs::parallel), and the total time is the slowest of the blocks, so roughly 2.2e-2, plus some overhead to copy local variables to shared memory with the mutex.
The time profile to form the Schur complement looks almost identical (with a small overhead for the parallel code), only that the overall time taken by the parallel code is divided by 2.
Remaining times are identical. Solve time is slightly lower for parallel code, but the impact is much smaller. Solving with schur complement (which uses dense lapack routines), is serial and potentially the most expensive part.
It makes sense then that the total time goes from 2.1 to 1.3 (38% less, compared to an ideal 50%).

BTW, switching off -fopenmp, the times still depend on OMP_NUM_THREADS. They shouldn't, but they do. I don't know why.
I think the best course of action to compare the results of MA86 and HFactor is to set OMP_NUM_THREADS to 1 and use the parallel code.
---
Checking the activity monitor, the behaviour makes sense:
If OMP_NUM_THREADS is 1 and I am using 2 Metis blocks with parallel code, the cpu saturates at 200%.
If OMP_NUM_THREADS is higher, then the cpu jumps higher (3-400% or more) when factorizing with MA86, goes down to 200% when assemlbying the Schur complement, goes to 100% when doing other serial stuff.
---


*****
22-11-23
I have done more tests to check that things behave properly and fixed some minor things.
I have separated the option for NLA and the option for metis.
Now 1 is AS, 2 is NE and -1 is choose (0 is the old cg). 0 is metis off, 1 is metis on, -1 is choose.
Added SelectMethod, to choose whether to use NE or AS (not yet implemented), whether or not to use Metis (if the Schur complement is small enough) and how many blocks to use (the one with smaller Schur complement).
Most of the times, 2 blocks will give the smallest Schur complement, but for some structured problems, it may be better to use more parts. Since it is cheap to run Metis, I run it with 2, 4 or 8 parts and check which one is better.
I should add an option for forcing the number of blocks. 
Changed the output to screen. It shows the selection and it gives a warning if the Schur complement is large.
---